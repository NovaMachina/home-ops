---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

tasks:

  generate-config:
    desc: Generate Talos configuration
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper genconfig
    preconditions:
      - test -f {{.TALOS_DIR}}/talconfig.yaml
      - test -f {{.ROOT_DIR}}/.sops.yaml
      - test -f {{.SOPS_AGE_KEY_FILE}}
      - which talhelper

  apply-node:
    desc: Apply Talos config to a node [IP=required]
    dir: '{{.TALOS_DIR}}'
    cmds:
      - task: down
      - talhelper gencommand apply --node {{.IP}} --extra-flags '--mode={{.MODE}}' | bash
      # shuffle a random ip, that way if the node being upgraded is the one picked we eventually pick one that's up
      - until talosctl --nodes $(talosctl config info --output yaml | yq --exit-status '.endpoints[]' | shuf -n 1) health; do sleep 5; done
      - task: up
    vars:
      MODE: '{{.MODE | default "auto"}}'
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.TALOSCONFIG}}
      - which talhelper talosctl yq

  upgrade-node:
    desc: Upgrade Talos on a single node [IP=required]
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand upgrade --node {{.IP}} --extra-flags "--image='{{.TALOS_IMAGE}}:{{.TALOS_VERSION}}' --timeout=10m" | bash
    vars:
      TALOS_IMAGE:
        sh: yq '.nodes[] | select(.ipAddress == "{{.IP}}") | .talosImageURL' {{.TALOS_DIR}}/talconfig.yaml
      TALOS_VERSION:
        sh: yq '.talosVersion' {{.TALOS_DIR}}/talenv.yaml
    requires:
      vars: [IP]
    preconditions:
      - talosctl --nodes {{.IP}} get machineconfig
      - talosctl config info
      - test -f {{.TALOSCONFIG}}
      - which kubectl talhelper talosctl yq

  upgrade-k8s:
    desc: Upgrade Kubernetes
    dir: '{{.TALOS_DIR}}'
    cmd: talhelper gencommand upgrade-k8s --extra-flags "--to '{{.KUBERNETES_VERSION}}'" | bash
    vars:
      KUBERNETES_VERSION:
        sh: yq '.kubernetesVersion' {{.TALOS_DIR}}/talenv.yaml
    preconditions:
      - talosctl config info
      - test -f {{.TALOSCONFIG}}
      - which talhelper talosctl yq

  apply-cluster:
    desc: Apply Talos config across the whole cluster [MODE=default]
    cmds:
      - for: { var: IP_ADDRS }
        task: apply-node
        vars:
          IP: '{{.ITEM}}'
          MODE: '{{.MODE}}'
    vars:
      MODE: '{{.MODE | default "auto"}}'
      IP_ADDRS:
        sh: talosctl config info --output json | jq --join-output '[.nodes[]] | join(" ")'
    preconditions:
      - which jq talosctl

  reset:
    desc: Resets nodes back to maintenance mode
    dir: '{{.TALOS_DIR}}'
    prompt: This will destroy your cluster and reset the nodes back to maintenance mode... continue?
    cmd: talhelper gencommand reset --extra-flags="--reboot {{- if eq .CLI_FORCE false }} --system-labels-to-wipe STATE --system-labels-to-wipe EPHEMERAL{{ end }} --graceful=false --wait=false" | bash
    preconditions:
      - which talhelper

  down:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl

  up:
    internal: true
    cmds:
      - until kubectl wait cephcluster --for=jsonpath=.status.ceph.health=HEALTH_OK --timeout=10m --all --all-namespaces &>/dev/null; do sleep 5; done
      - until kubectl wait jobs --all --all-namespaces --for=condition=complete --timeout=5m &>/dev/null; do sleep 5; done
    preconditions:
      - which kubectl
